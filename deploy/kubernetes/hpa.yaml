apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: memory-graph-hpa
  namespace: llm-memory-graph
  labels:
    app: memory-graph
    component: autoscaling
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: memory-graph
  minReplicas: 3
  maxReplicas: 10

  # Scale-up/down behavior
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # 5 minutes
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 1
        periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 60  # 1 minute
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 2
        periodSeconds: 30
      selectPolicy: Max

  # Metrics for scaling decisions
  metrics:
  # CPU-based scaling
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

  # Memory-based scaling
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80

  # Custom metric: gRPC requests per second
  # Requires Prometheus Adapter or similar metrics server
  - type: Pods
    pods:
      metric:
        name: grpc_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"

  # Custom metric: gRPC request latency (p95)
  # Scale up if p95 latency exceeds 100ms
  - type: Pods
    pods:
      metric:
        name: grpc_request_duration_p95
      target:
        type: AverageValue
        averageValue: "100m"  # 100 milliseconds

  # Custom metric: Active gRPC streams
  - type: Pods
    pods:
      metric:
        name: grpc_active_streams
      target:
        type: AverageValue
        averageValue: "100"

---
# Note: For custom metrics to work, you need:
# 1. Prometheus Adapter installed
# 2. ServiceMonitor configured (see servicemonitor.yaml)
# 3. Metrics properly exposed by the application
#
# Example Prometheus Adapter configuration:
# apiVersion: v1
# kind: ConfigMap
# metadata:
#   name: adapter-config
#   namespace: monitoring
# data:
#   config.yaml: |
#     rules:
#     - seriesQuery: 'memory_graph_grpc_requests_total'
#       resources:
#         overrides:
#           namespace: {resource: "namespace"}
#           pod: {resource: "pod"}
#       name:
#         matches: "^memory_graph_grpc_requests_total$"
#         as: "grpc_requests_per_second"
#       metricsQuery: 'rate(memory_graph_grpc_requests_total{<<.LabelMatchers>>}[1m])'
#
#     - seriesQuery: 'memory_graph_grpc_request_duration_seconds'
#       resources:
#         overrides:
#           namespace: {resource: "namespace"}
#           pod: {resource: "pod"}
#       name:
#         matches: "^memory_graph_grpc_request_duration_seconds$"
#         as: "grpc_request_duration_p95"
#       metricsQuery: 'histogram_quantile(0.95, sum(rate(memory_graph_grpc_request_duration_seconds_bucket{<<.LabelMatchers>>}[1m])) by (le, pod))'
#
#     - seriesQuery: 'memory_graph_grpc_active_streams'
#       resources:
#         overrides:
#           namespace: {resource: "namespace"}
#           pod: {resource: "pod"}
#       name:
#         matches: "^memory_graph_grpc_active_streams$"
#         as: "grpc_active_streams"
#       metricsQuery: 'memory_graph_grpc_active_streams{<<.LabelMatchers>>}'

---
# Optional: VerticalPodAutoscaler for automatic resource tuning
# Requires VPA installed in cluster
# apiVersion: autoscaling.k8s.io/v1
# kind: VerticalPodAutoscaler
# metadata:
#   name: memory-graph-vpa
#   namespace: llm-memory-graph
# spec:
#   targetRef:
#     apiVersion: apps/v1
#     kind: Deployment
#     name: memory-graph
#   updatePolicy:
#     updateMode: "Auto"
#   resourcePolicy:
#     containerPolicies:
#     - containerName: memory-graph
#       minAllowed:
#         cpu: 500m
#         memory: 1Gi
#       maxAllowed:
#         cpu: 4000m
#         memory: 8Gi
